<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <script type="application/ld+json">

{  
  "@context":"http://schema.org",
  "@type":"Website",
  "@id":"https:\/\/moisesvw.github.io\/",
  "author": {
    "@type": "Person",
    "name": "Moisés Vargas",
    
  },
  "name":"Machine Learning Stuffs",
  "description":"Machine Learning Engineer Nanodegree Algorithms and Techniques Convolutional Neural Network (ConvNet) as classifier. The network has several parameters that needs to be tuned, including number of epochs, batch size, learning rate, max pool layers and dropout layers. This technique and its parameters are discussed above.\nThe ConvNet is trained using back-propagation to learn the weight parameters, these weights are used to activate neurons. The main difference of the ConvNet with respect to a regular neural network is that it is specifically designed to recognize visual patterns directly from the image pixels [4].",
  "url":"https:\/\/moisesvw.github.io\/2017\/november\/deep-learning-on-lenet5\/",
  "keywords":"[, machine-learning, self-driving-cars]"
}

</script>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.110.0 with theme Tranquilpeak 0.5.3-BETA">
<meta name="author" content="Moisés Vargas">
<meta name="keywords" content=", machine-learning, self-driving-cars">
<meta name="description" content="Machine Learning Engineer Nanodegree Algorithms and Techniques Convolutional Neural Network (ConvNet) as classifier. The network has several parameters that needs to be tuned, including number of epochs, batch size, learning rate, max pool layers and dropout layers. This technique and its parameters are discussed above.
The ConvNet is trained using back-propagation to learn the weight parameters, these weights are used to activate neurons. The main difference of the ConvNet with respect to a regular neural network is that it is specifically designed to recognize visual patterns directly from the image pixels [4].">


<meta property="og:description" content="Machine Learning Engineer Nanodegree Algorithms and Techniques Convolutional Neural Network (ConvNet) as classifier. The network has several parameters that needs to be tuned, including number of epochs, batch size, learning rate, max pool layers and dropout layers. This technique and its parameters are discussed above.
The ConvNet is trained using back-propagation to learn the weight parameters, these weights are used to activate neurons. The main difference of the ConvNet with respect to a regular neural network is that it is specifically designed to recognize visual patterns directly from the image pixels [4].">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning on LeNet5">
<meta name="twitter:title" content="Deep Learning on LeNet5">
<meta property="og:url" content="https://moisesvw.github.io/2017/november/deep-learning-on-lenet5/">
<meta property="twitter:url" content="https://moisesvw.github.io/2017/november/deep-learning-on-lenet5/">
<meta property="og:site_name" content="Machine Learning Stuffs">
<meta property="og:description" content="Machine Learning Engineer Nanodegree Algorithms and Techniques Convolutional Neural Network (ConvNet) as classifier. The network has several parameters that needs to be tuned, including number of epochs, batch size, learning rate, max pool layers and dropout layers. This technique and its parameters are discussed above.
The ConvNet is trained using back-propagation to learn the weight parameters, these weights are used to activate neurons. The main difference of the ConvNet with respect to a regular neural network is that it is specifically designed to recognize visual patterns directly from the image pixels [4].">
<meta name="twitter:description" content="Machine Learning Engineer Nanodegree Algorithms and Techniques Convolutional Neural Network (ConvNet) as classifier. The network has several parameters that needs to be tuned, including number of epochs, batch size, learning rate, max pool layers and dropout layers. This technique and its parameters are discussed above.
The ConvNet is trained using back-propagation to learn the weight parameters, these weights are used to activate neurons. The main difference of the ConvNet with respect to a regular neural network is that it is specifically designed to recognize visual patterns directly from the image pixels [4].">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2017-11-01T00:00:00">
  
  
    <meta property="article:modified_time" content="2017-11-01T00:00:00">
  
  
  
  


<meta name="twitter:card" content="summary">

  <meta name="twitter:site" content="@moisesvw">


  <meta name="twitter:creator" content="@moisesvw">











    <title>Deep Learning on LeNet5</title>

    <link rel="icon" href="https://moisesvw.github.io/robot-network-icon.png">
    

    

    <link rel="canonical" href="https://moisesvw.github.io/2017/november/deep-learning-on-lenet5/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    
    
    
    <link rel="stylesheet" href="https://moisesvw.github.io/css/style-h6ccsoet3mzkbb0wngshlfbaweimexgqcxj0h5hu4h82olsdzz6wmqdkajm.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-92619527-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="2">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/" aria-label="Go to homepage">Machine Learning Stuffs</a>
  </div>
  
    
      <a class="header-right-picture "
         href="/#about" aria-label="Open the link: /#about">
    
    
    
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="2">
  <div class="sidebar-container">
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/archives" title="Archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/moisesvw" target="_blank" rel="noopener" title="GitHub">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/moisesvw/" target="_blank" rel="noopener" title="LinkedIn">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://twitter.com/moisesvw" target="_blank" rel="noopener" title="Twitter">
    
      <i class="sidebar-button-icon fa fa-lg fa-twitter"></i>
      
      <span class="sidebar-button-desc">Twitter</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="2"
        class="
               hasCoverMetaIn
               ">
        <article class="post" id="top">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title">
      Deep Learning on LeNet5
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time datetime="2017-11-01T00:00:00-05:00">
        
  November 1, 2017

      </time>
    
    
  </div>

</div>
          
          <div class="post-content markdown">
            <div class="main-content-wrap">
              <h2 id="machine-learning-engineer-nanodegree">Machine Learning Engineer Nanodegree</h2>
<h3 id="algorithms-and-techniques">Algorithms and Techniques</h3>
<p>Convolutional Neural Network (ConvNet) as classifier. The network has several parameters that needs to be tuned, including  number of epochs, batch size, learning rate, max pool layers and dropout layers. This technique and its parameters are discussed above.</p>
<p>The ConvNet is trained using back-propagation to learn the weight parameters, these weights are used to activate neurons. The main difference of the ConvNet with respect to a regular neural network is that it is  specifically designed to recognize visual patterns directly from the image pixels [4].</p>
<p><img src="/img/lenet-5.png" alt="alt text" title="LeNet-5">
Figure-7 Source: Yan LeCun</p>
<p>The figure above shows the LeNet-5 architecture that is intended to be used in this project as the baseline model. The next paragraphs explain how this neural network architecture works when it receives the image and how the image is processed through the network until  the output.</p>
<table>
<thead>
<tr>
<th>Layer name</th>
<th style="text-align:right">Size</th>
<th style="text-align:right">Channels</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td style="text-align:right">32x32</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td>Convolution 1 (C1)</td>
<td style="text-align:right">28x28</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Sub-sampling</td>
<td style="text-align:right">14x14</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Convolution 2 (C2)</td>
<td style="text-align:right">10x10</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td>Sub-sampling</td>
<td style="text-align:right">5x5</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td>Fully connect (FC1)</td>
<td style="text-align:right">120</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Fully connect (FC2)</td>
<td style="text-align:right">84</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Output</td>
<td style="text-align:right">10</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Table-1 LeNet5 architecture</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The table above denotes the architecture of LeNet-5, the input layer  is an image. In this example it is represented as  digits comprising images of 32x32 pixels. Here it can be assumed that the image is grayscale such that it has one channel. The dataset of this project are images with 3 channels of color space, for example, Red, Green and Blue.</p>
<p>The first Convolution layer (C1) it is made by applying a convolution that basically scans the image with a patch of smaller size, in this case the patch is 5x5 pixels in size and applying an activation function on it. This patch is applied over the image horizontally and vertically. Once this process finishes, the result will be an image of 28x28. This process will happen six times building up the first convolution layer for the given image. Finally the dimensions of the (C1) layer will be 6@(channels)28x28. This layer represents the automatic extraction of features from the image that was fed into the network.</p>
<p>The second layer is a sub-sampling (S2) that will have the same number of channels as the layer before and the features will be reduced by half, ending with dimensions 6@14x14. This means that a given image after passing the (C1) will end up in (S2) with size of 14x14 pixels and 6 transformations that came from the (C1) layer. The sub-sampling uses a function that is applied together with a patch, in this case the patch size is 2x2 and the function could be a Max function, for example, when you apply a Max function with patch size 2x2 to an image of 28x28 pixels, every patch will choose one pixel with the max value. This process applied to a 28x28 image will occur 196 times and returns an output of size 14x14. This kind of layer aims to reduce the space of feature while retaining the relevant features.</p>
<p>The third layer (C3) it is a convolution similar to the first (C1) but with size of 16@10x10 as a result of applying a patch of size 5x5 pixels. Fourth layer (S4) same as (S2) at this point applied to (C3) ending with size of 16@5x5.</p>
<p>Finally this network has two fully connected layers (FC5), its input is an unrolled version of layer (S4) which is a vector of 400 features (16x5x5) and its output is a vector of 100 features. Next, the sixth layer (FC6) receives as input a vector of 100 features and returns a vector of 84 features. The output layer receives the 84 features vector and returns a vector of 10 probabilities, the value of each position of the vector represents the probability of that position to be the class that represent the original image that was fed in the input layer.</p>
<p>Beside  the convolution and Sub-sampling layers described above, there is another layer in this project call Dropout. The objective of this layer is to avoid overfitting the data in the network by randomly turning off connections between layers.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th style="text-align:right">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>EPOCHS</td>
<td style="text-align:right">Number of training passes</td>
</tr>
<tr>
<td>BATCH_SIZE</td>
<td style="text-align:right">Batch size in training</td>
</tr>
<tr>
<td>learning rate</td>
<td style="text-align:right">Initial rate for the optimizer</td>
</tr>
<tr>
<td>Table-2 Model parameters</td>
<td></td>
</tr>
</tbody>
</table>
<p>Additional parameters</p>
<p>Neural networks in general work with weights that are the main parameters of the network &ndash; they works as the neuron connections. The objective of the network in this case is to classify which type of cervix is present in an image. A good result in this task depends on how well these weight parameters get tuned by learning.</p>
<p>All parts of the network that were discussed above (convolution layer with activation functions, sub-sampling layers, dropout layers and fully connected layers) are the foundation structure that helps to convey the learning to the network’s weights.
The missing piece of this puzzle is the conveyor, which is the technique that iterates through the network, updating the weights, and this is controlled by the parameters listed in the table-5. The conveyor that is used by the ConvNet in this project is well known as AdamOptimizer. It, as well as others optimizers, uses Back-propagation to optimize the values of the weights that better optimize the neural network.</p>
<p>Because the ConvNet needs image data and usually this kind of data consumes a large  amount of computational memory, it is necessary to train the network using batches, that means passing a subset of data to the network in the training process. To control the batch sizes, this project uses the parameter Batch Size. For instance if the data contains 8000 images, using a batch size of 64 produces 125 batches in training.
The Epoch parameter helps to control how long the training operation will take. For instance, an epoch of 3 and batch size of 64 with 8000 examples of training will cause the optimizer to learn in 375(3*125) batches, for each batch, the network and the optimizer need to digest 64 images to adjust the weights.</p>
<p>Finally, the learning rate is an initial parameter of the optimizer. It is optional because one characteristic of AdamOptimizer is that it automatically initializes and updates this learning rate. However, controlling this parameter by providing an initial value will affect the learning results by controlling how fast or slow the optimizer can converge to discover the weights.</p>
<h3 id="benchmark">Benchmark</h3>
<p>The benchmark model proposed in this project is the LeNet-5 architecture outlined in table-4 with a minor modification in the output layer to predict 3 different classes instead of 10. The benchmark uses the configuration of table-7. According with the metrics defined in Metrics section, the table-8 reports the measurements of the baseline model that will be used to compare with the final model. The data for the baseline model is randomly split in 60% for training 28% and 12% for test.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th style="text-align:right">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>EPOCHS</td>
<td style="text-align:right">15</td>
</tr>
<tr>
<td>BATCH_SIZE</td>
<td style="text-align:right">128</td>
</tr>
<tr>
<td>learning rate</td>
<td style="text-align:right">0.01</td>
</tr>
<tr>
<td>Table-3 base model parameters</td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Metric</th>
<th style="text-align:right">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train accuracy</td>
<td style="text-align:right">0.654</td>
</tr>
<tr>
<td>Validation accuracy</td>
<td style="text-align:right">0.553</td>
</tr>
<tr>
<td>Test Accuracy</td>
<td style="text-align:right">0.551</td>
</tr>
<tr>
<td>Table-4 Baseline model metrics</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="iii-methodology">III. Methodology</h2>
<p>Being able to control the size of the image data when training ConvNet will be useful since it can reduce the amount of computation memory and the training time that the model can take. Also normalizing image data to be zero centered will help the model to converge better.</p>
<pre tabindex="0"><code>def resize_image(img, size=(820, 620), debug=False):
    &#34;&#34;&#34;
    resize image and return it
    &#34;&#34;&#34;
    h = size[0]
    w = size[1]
    img_ = cv2.resize(img, (w, h))
    return img_
</code></pre><pre tabindex="0"><code>def scale_image(img):
    &#34;&#34;&#34;
    normalize the image
    &#34;&#34;&#34;
    return img / 255.0 - 0.5
</code></pre><p>A function that receives an image and returns it as the model required was implemented to facilitate the image transformation. The code below shows this function named lamba_layer which receives an image, creates a copy, applies the resize function, changes the image color space(in this example this image is being converted from RGB to Gray), and finally  scales the image before returning it  to the model.</p>
<pre tabindex="0"><code>def lambda_layer(image):
    &#34;&#34;&#34;
    pre-process the image before pass it to the
    ConvNet
    &#34;&#34;&#34;
    img_ = np.copy(image)
    img_ = resize_image(img_, (H_IMG, W_IMG))
    img_ = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)
    img_ = img_[:, :, np.newaxis]
    img_ = scale_image(img_)
    return img_
</code></pre><h3 id="implementation">Implementation</h3>
<p>The implementation use Pandas to store the information for the images , OpenCV to read images from file system, scikit-learn as a helper to split the data into training, validation and tests sets, and TensorFlow to implement the ConvNet.</p>
<p>This project use GPU-GRID K520 from AWS with 4GB of memory, however, not all of the data can be fed into the ConvNet because it will cause memory resource exhaustion. For that reason, it was necessary to implement a function generator to lazily generate batches of images that will be passed through the ConvNet. The code below shows how this function is implemented. The function receives a Pandas dataframe that contains images path and type, then maps this input to a randomized set of images that will be preprocessed. Each time this function is called, it will generate a set of images and labels according with the batch size.</p>
<pre tabindex="0"><code>def generator(samples,batch_size=128):
    &#34;&#34;&#34;
    Generates random batches of data base on image path
    &#34;&#34;&#34;

    num_samples = len(samples)
    while True:
        randomsamples = samples.sample(frac=1)
        for offset in range(0, num_samples, batch_size):
            batch_samples = samples[offset:offset+batch_size]
            
            images = []
            labels = []
            for _, row in batch_samples.iterrows():
                image_path = row[&#39;image&#39;].strip()

                image = cv2.imread(image_path)
                label = row[&#39;target&#39;]

                images.append(lambda_layer(image))
                labels.append(label)

            X_train = np.array(images)
            y_train = np.array(labels)

            yield sklearn.utils.shuffle(X_train, y_train)
</code></pre><p>The evaluate function performs accuracy operations within a Tensorflow session and returns how well the model accuracy performs.</p>
<pre tabindex="0"><code>def evaluate(sess, generator, steps, accuracy_op, dict_opts={}):
    total_examples = 0
    total_accuracy = 0

    for step in range(0, steps):
        batch_x, batch_y = next(generator)
        accuracy = sess.run(accuracy_op, feed_dict={ **{ x: batch_x, y: batch_y }, **dict_opts})
        num_examples = len(batch_x)
        total_accuracy += (accuracy * num_examples)
        total_examples += num_examples
    return total_accuracy / total_examples
</code></pre><p>The test_model function evaluates a stored model on the test set and reports its accuracy.</p>
<pre tabindex="0"><code>def test_model(test_gen, test_steps, accuracy_op, saver, dict_opts={}, name_model=&#34;.&#34;):
    &#34;&#34;&#34;
    Test a saved model with test data
    &#34;&#34;&#34;
    if not os.path.exists(name_model+&#34;.index&#34;):
        print(name_model, &#34;Model does not exists in disk&#34;)
        return False
    with tf.Session() as sess:
        saver.restore(sess, name_model)
        test_accuracy = evaluate(sess, test_gen, test_steps, accuracy_op, dict_opts=dict_opts )
        print(&#34;Test Accuracy = {:.3f}&#34;.format(test_accuracy))
</code></pre><p>The following function with name “training_validation” was created to perform training and validation of the model. The next paragraphs will explain every parameter that this function receive in order to make sense how the model is being trained.</p>
<p>Function “training_validation” parameters:</p>
<ul>
<li>
<p>name_model:
The name of the model this will be used to store the model in disk and print results</p>
</li>
<li>
<p>train_gen:
As explained above this is an initialized generator function with training data</p>
</li>
<li>
<p>val_gen:
As explained above this is an initialized generator function with validation data</p>
</li>
<li>
<p>train_steps and val_steps:
Number of batches which contains both training and validation sets</p>
</li>
<li>
<p>train_op:
Tensorflow operation that minimize the loss function of the model.</p>
</li>
<li>
<p>accuracy_op:
Tensorflow operation that calculate the accuracy of the model</p>
</li>
<li>
<p>saver:
Object that can be used to store or retrieve a model</p>
</li>
<li>
<p>train_opts:
Options that can be passed to the feed dictionary of tensor training operation</p>
</li>
<li>
<p>dict_ops:
Options that can be passed to the feed dictionary of tensor for validation or test operations</p>
</li>
<li>
<p>old_train and old_model:
To control whether yes or not the training should be on top of a pre trained model</p>
</li>
<li>
<p>store:
To control whether yes or not store the final model</p>
</li>
<li>
<p>epochs:
Number of epochs the training will perform</p>
</li>
</ul>
<p>The training_validation function stores the model in each epoch such that the model can be retrained from one of the epoch results.</p>
<pre tabindex="0"><code>def training_validation(name_model, train_gen, val_gen, train_steps,
                        val_steps, train_op, accuracy_op, saver,
                        train_opts={}, dict_ops={}, old_train=False,
                        old_model=&#39;&#39;, store=False, epochs=3):
    &#34;&#34;&#34;
    Train a model, store the model in disk and return the path
    &#34;&#34;&#34;

    return_data = {
       &#39;save_path&#39;: &#39;./&#39;+name_model
    }

    with tf.Session() as sess:
        if old_train:
            if not os.path.exists(&#39;./&#39;+old_model+&#34;.index&#34;):
                print(old_model, &#34;Model does not exists in disk&#34;)
                return return_data
            saver.restore(sess, &#39;./&#39;+old_model)
        else:
            sess.run(tf.global_variables_initializer())

        print(name_model, &#34;Training...&#34;)
        print()

        for i in range(epochs):
            for step in range(0, train_steps):
                batch_x, batch_y = next(train_gen)

                sess.run(train_op, feed_dict={**{x: batch_x, y: batch_y}, **train_opts })
            
            train_accu = evaluate(sess, train_gen, train_steps, accuracy_op, dict_opts=dict_ops)
            val_accu = evaluate(sess, val_gen, val_steps, accuracy_op, dict_opts=dict_ops)

            #Save trainning per epoch
            saver.save(sess, &#39;./&#39;+name_model+str(i+1))
            print(&#34;EPOCH {} ...&#34;.format(i+1))
            print(&#34;Train Accuracy = {:.3f}&#34;.format(train_accu))
            print(&#34;Validation Accuracy = {:.3f}&#34;.format(val_accu))
            print()


            
        if store:
            saver.save(sess, return_data[&#39;save_path&#39;])
            print(name_model, &#34;Model saved&#34;)
        
    return return_data
</code></pre><p>The LeNet-5 architecture discussed in this project is implemented in the following function using Tensorflow.</p>
<pre tabindex="0"><code>def LeNet5(x, init_channels=1, fcx0_len=400):
    mu = 0
    sigma = 0.1
    
    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, init_channels, 6), mean = mu, stddev = sigma))
    conv1_b = tf.Variable(tf.zeros(6))
    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) + conv1_b

    conv1 = tf.nn.relu(conv1)

    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;VALID&#39;)

    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))
    conv2_b = tf.Variable(tf.zeros(16))
    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) + conv2_b
    
    conv2 = tf.nn.relu(conv2)

    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;VALID&#39;)

    fc0   = flatten(conv2)

    fc1_W = tf.Variable(tf.truncated_normal(shape=(fc0_len, 120), mean = mu, stddev = sigma))
    fc1_b = tf.Variable(tf.zeros(120))
    fc1   = tf.matmul(fc0, fc1_W) + fc1_b
    fc1   = tf.nn.relu(fc1)

    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))
    fc2_b  = tf.Variable(tf.zeros(84))
    fc2    = tf.matmul(fc1, fc2_W) + fc2_b
    
    fc2    = tf.nn.relu(fc2)

    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 3), mean = mu, stddev = sigma))
    fc3_b  = tf.Variable(tf.zeros(3))
    logits = tf.matmul(fc2, fc3_W) + fc3_b

    return logits
</code></pre><p>The hyper parameters were initialized as follow</p>
<pre tabindex="0"><code>BATCH_SIZE = 128
H_IMG, W_IMG = (32,32)
EPOCHS = 6
rate = 0.01
</code></pre><p>The following code shows how the data generators were initialized. Since this project contemplates that a training can start from an older, trained model, the train set, validation set and test set were stored in csv files and are retrieved from these files to be consistent with this data over all training process.</p>
<pre tabindex="0"><code>train_samples, validation_samples = train_test_split(data, test_size=0.4)
validation_samples, test_samples  = train_test_split(validation_samples, test_size=0.3)

train_generator = generator(train_samples, batch_size=BATCH_SIZE)
validation_generator = generator(validation_samples, batch_size=BATCH_SIZE)
test_generator = generator(test_samples, batch_size=BATCH_SIZE)

train_steps      = math.ceil(len(train_samples)/BATCH_SIZE)
validation_steps = math.ceil(len(validation_samples)/BATCH_SIZE)
test_steps       = math.ceil(len(test_samples)/BATCH_SIZE)
</code></pre><p>The Tensorflow operations that use LeNet-5 architecture are shown in the following code:</p>
<pre tabindex="0"><code>x = tf.placeholder(tf.float32, (None, 32, 32, 1))
y = tf.placeholder(tf.int32, (None))
one_hot_y = tf.one_hot(y, 3)

logits = LeNet5(x)
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)
loss_operation = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(learning_rate=rate)
training_operation = optimizer.minimize(loss_operation)
correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))
accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre><p>As discussed above, the function training_validation receives the training_operation and accuracy_operation and the other parameters previously described to train the model.</p>
<pre tabindex="0"><code>saver = tf.train.Saver()
lenet_5_data = training_validation(&#39;lenet5&#39;, train_generator, validation_generator,
                                   train_steps, validation_steps, training_operation,
                                   accuracy_operation, saver, train_opts={}, dict_ops={},
                                   old_train=False, store=True, epochs=EPOCHS)
</code></pre><p>finally to test the model the next code was used</p>
<pre tabindex="0"><code>test_model(test_generator, test_steps, accuracy_operation, saver, dict_opts={}, name_model=lenet_5_data[&#39;save_path&#39;])
</code></pre><h3 id="refinement">Refinement</h3>
<p>This section explains how the solution model based on LeNet-5 was built by applying modifications to the network in order to increase the performance of evaluation metrics that indicate better results than the benchmark model. Different experiments were performed with combinations of settings  specific to the Convolutional Neural Networks, for example,  adding or removing filters from convolutions, adding convolution layers, changing the patch size of the convolution, etc. In addition, experimentation with dropout layers and pooling layers are considered in this section to improve model performance.</p>
<p>Experimentation using other color spaces for the images, like RGB, HSV, YCrCb, are considered, as well as the normalization of the images to be zero centered.
After experimentation, the best model with best accuracy on training, validation and test sets will be chosen as finally solution model.</p>
<p>The first model solution was based on LeNet-5 described in Algorithms and Techniques section, table-5 shows the architecture and table-6 the results of this architecture.</p>
<table>
<thead>
<tr>
<th>Layer name</th>
<th style="text-align:right">Size</th>
<th style="text-align:right">Channels</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td style="text-align:right">155x155</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td>Convolution 1 (C1)</td>
<td style="text-align:right">151x151</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Sub-sampling</td>
<td style="text-align:right">75x75</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Convolution 2 (C2)</td>
<td style="text-align:right">71x71</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td>Sub-sampling</td>
<td style="text-align:right">35x35</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td>Fully connect (FC1)</td>
<td style="text-align:right">120</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Fully connect (FC2)</td>
<td style="text-align:right">84</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Output</td>
<td style="text-align:right">3</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Table-5 LeNet-5</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The ConvNet outlined in table-9 was trained using the following parameters using different color spaces and obtaining the results that shows table-10</p>
<p>Parameters:</p>
<ul>
<li>EPOCHS = 10</li>
<li>learning rate = 0.01</li>
</ul>
<table>
<thead>
<tr>
<th>Color Space</th>
<th style="text-align:right">Train</th>
<th style="text-align:right">Validation</th>
<th style="text-align:right">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>RGB 3 channels</td>
<td style="text-align:right">0.335</td>
<td style="text-align:right">0.336</td>
<td style="text-align:right">0.320</td>
</tr>
<tr>
<td>HSV 3 channels</td>
<td style="text-align:right">0.766</td>
<td style="text-align:right">0.514</td>
<td style="text-align:right">0.523</td>
</tr>
<tr>
<td>YCrCb 3 channels</td>
<td style="text-align:right">0.748</td>
<td style="text-align:right">0.504</td>
<td style="text-align:right">0.483</td>
</tr>
<tr>
<td>Table-6 Report first solution</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>As  table-6 shows, the best model in the first solution is the one that use HSV Color Space. Since this first solution model does not show big improvements based on the benchmark model, the next steps of improvement were to decrease the learning rate, increase number of epochs and add neurons in fully connected layers FC1 from 120 to 1000 and FC2 from 84 to 250 neurons.</p>
<p>The table-7 shows the results of the model evaluated in the three color spaces. The model with YCrCb color space performs better that the other two, however it shows signs of overfitting.</p>
<p>Parameters:</p>
<ul>
<li>EPOCHS = 10</li>
<li>learning rate = 0.0008</li>
</ul>
<table>
<thead>
<tr>
<th>Color Space</th>
<th style="text-align:right">Train</th>
<th style="text-align:right">Validation</th>
<th style="text-align:right">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>RGB 3 channels</td>
<td style="text-align:right">0.997</td>
<td style="text-align:right">0.641</td>
<td style="text-align:right">0.619</td>
</tr>
<tr>
<td>HSV 3 channels</td>
<td style="text-align:right">0.979</td>
<td style="text-align:right">0.648</td>
<td style="text-align:right">0.630</td>
</tr>
<tr>
<td>YCrCb 3 channels</td>
<td style="text-align:right">0.985</td>
<td style="text-align:right">0.646</td>
<td style="text-align:right">0.623</td>
</tr>
<tr>
<td>Table-7 Report solution first improvement</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The next improvement was to add dropout layers next to the fully connected layers FC1 and FC2 with value of (0.5) and increased number of epochs to 20. The results are shown in Table-8. This experiment results still suffers of overfitting despite the dropout layers.</p>
<p>Parameters:</p>
<ul>
<li>EPOCHS = 20</li>
<li>learning rate = 0.0008</li>
</ul>
<table>
<thead>
<tr>
<th>Color Space</th>
<th style="text-align:right">Train</th>
<th style="text-align:right">Validation</th>
<th style="text-align:right">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>RGB 3 channels</td>
<td style="text-align:right">0.997</td>
<td style="text-align:right">0.648</td>
<td style="text-align:right">0.620</td>
</tr>
<tr>
<td>HSV 3 channels</td>
<td style="text-align:right">0.999</td>
<td style="text-align:right">0.660</td>
<td style="text-align:right">0.663</td>
</tr>
<tr>
<td>YCrCb 3 channels</td>
<td style="text-align:right">0.997</td>
<td style="text-align:right">0.654</td>
<td style="text-align:right">0.641</td>
</tr>
<tr>
<td>Table-8 Report solution second improvement</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Other experiments that were performed are shown in table-9, the intention was to explore different channels of color spaces and some combinations looking for decrease the overfitting. The kernel sizes of the pooling layers will be increased. Only HSV and YCrCb color spaces are considering for this experiment Table-9 shows the results.</p>
<p>Parameters:</p>
<ul>
<li>EPOCHS = 20</li>
<li>learning rate = 0.0008</li>
</ul>
<table>
<thead>
<tr>
<th>Color Space</th>
<th style="text-align:right">Train</th>
<th style="text-align:right">Validation</th>
<th style="text-align:right">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>HSV(Hue)</td>
<td style="text-align:right">0.912</td>
<td style="text-align:right">0.626</td>
<td style="text-align:right">0.624</td>
</tr>
<tr>
<td>HSV(Saturation)</td>
<td style="text-align:right">0.960</td>
<td style="text-align:right">0.672</td>
<td style="text-align:right">0.656</td>
</tr>
<tr>
<td>HSV(H+S)</td>
<td style="text-align:right">0.954</td>
<td style="text-align:right">0.676</td>
<td style="text-align:right">0.658</td>
</tr>
<tr>
<td>HSV(S+B)</td>
<td style="text-align:right">0.970</td>
<td style="text-align:right">0.683</td>
<td style="text-align:right">0.637</td>
</tr>
<tr>
<td>YCrCb(Y)</td>
<td style="text-align:right">0.976</td>
<td style="text-align:right">0.648</td>
<td style="text-align:right">0.639</td>
</tr>
<tr>
<td>YCrCb(Cr)</td>
<td style="text-align:right">0.925</td>
<td style="text-align:right">0.670</td>
<td style="text-align:right">0.630</td>
</tr>
<tr>
<td>YCrCb(Cb)</td>
<td style="text-align:right">0.924</td>
<td style="text-align:right">0.668</td>
<td style="text-align:right">0.661</td>
</tr>
<tr>
<td>YCrCb(Y+Cr)</td>
<td style="text-align:right">0.921</td>
<td style="text-align:right">0.658</td>
<td style="text-align:right">0.615</td>
</tr>
<tr>
<td>YCrCb(Cr+Cb)</td>
<td style="text-align:right">0.946</td>
<td style="text-align:right">0.686</td>
<td style="text-align:right">0.665</td>
</tr>
<tr>
<td>Table-9</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The results above indicate that the best model, according to the validation and test scores, is the one that uses HSV color space with H+S channels. All of the experimented models show strong signs of overfitting despite the effort to reduce it. In order to improve on the selected model, five re-trainings with 20 epochs each were performed,  keeping the learned weights of the previous trainings. However, the results were not different and the overfitting remained the same.</p>
<p>This project considered to continue the evaluation of this model despite the overfitting due to time restriction. The model architecture obtained after perform the experiments is outlined in
Table-10.</p>
<table>
<thead>
<tr>
<th>Layer name</th>
<th style="text-align:right">Size</th>
<th style="text-align:right">Channels</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td style="text-align:right">155x155</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td>Convolution 1 (C1)</td>
<td style="text-align:right">149x149</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Sub-sampling</td>
<td style="text-align:right">73x73</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Convolution 2 (C2)</td>
<td style="text-align:right">69x69</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td>Sub-sampling</td>
<td style="text-align:right">32x32</td>
<td style="text-align:right">16</td>
</tr>
<tr>
<td>Fully connect (FC1)</td>
<td style="text-align:right">1000</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Dropout(0.5)</td>
<td style="text-align:right">-</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Fully connect (FC2)</td>
<td style="text-align:right">250</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Dropout(0.5)</td>
<td style="text-align:right">-</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Output</td>
<td style="text-align:right">3</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Table-10 Model architecture HSV (H+S)</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Since the first solution based on LeNet 5 did not bring the expected results, another approach call transfer learning using ConvNet architecture AlexNet [7] was used as an improvement. AlexNet is a pre-trained neural network that contains 60 million parameters and 650,000 neurons.</p>
<p>The architecture of AlexNet is shown in Figure-8, it has five convolution layers, some max-pooling layers and three fully connected layers. This network was trained to classify 1.2 million images that can fall down into one of one thousand different categories.</p>
<p><img src="/img/alex-net.png" alt="alt text" title="AlexNet">
Figure-8 Source: AlexNet [7]</p>
<p>For this experiment the pre-trained AlexNet model was used with its convolution and two fully connected layers, the fully connected layers and output layers from the previous model LetNet-5 were appended to the last fully connected layer of AlexNet. The Table-11 shows the architecture of the new model.</p>
<table>
<thead>
<tr>
<th>Layer name</th>
<th style="text-align:right">Size</th>
<th style="text-align:right">Channels</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td style="text-align:right">155x155</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td>AlexNet Pretrained</td>
<td style="text-align:right">4096</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Fully connect (FC1)</td>
<td style="text-align:right">1000</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Dropout(0.5)</td>
<td style="text-align:right">-</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Fully connect (FC2)</td>
<td style="text-align:right">250</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Output</td>
<td style="text-align:right">3</td>
<td style="text-align:right">-</td>
</tr>
<tr>
<td>Table-11 AlexNet pretrained model</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>With the model outlined above, several experiments were performed using the same color space as other experiments namely RGB, HSV and YCrCb. AlexNet performs better with RGB and the parameters above were used to obtain results of Table-12</p>
<p>Parameters:</p>
<ul>
<li>EPOCHS = 100</li>
<li>learning rate = 0.0008</li>
<li>Color Space RGB</li>
</ul>
<table>
<thead>
<tr>
<th>Metrics</th>
<th style="text-align:right">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td style="text-align:right">0.959</td>
</tr>
<tr>
<td>Validation</td>
<td style="text-align:right">0.697</td>
</tr>
<tr>
<td>Test</td>
<td style="text-align:right">0.659</td>
</tr>
<tr>
<td>Table-12 Report solution AlexNet</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="reflection">Reflection</h3>
<p>Reaching a final solution was a process that took a long time of experimentation with different settings to try to overcome the overfitting of data.</p>
<p>The process can be summarized in the following steps:</p>
<h5 id="data-exploration">Data Exploration</h5>
<p>The data was downloaded from Kaggle, uncompressed and analyzed. I found that it was necessary to normalize the image sizes to be consistent with the input of the model that will process it. During the process, different color space were explored and then applied to the model.</p>
<p>In the exploration imaging tools like ImageMagick were applied to the image files to detect corrupt data and a few samples were found</p>
<h5 id="data-modeling">Data Modeling</h5>
<p>At the beginning, it was clear that applying Convolutional Neural Network to images classification problem is probably a good fit to find out a good solution. This project started out on LeNet-5 Architecture, a well known Convolutional Neural Network that has gotten good results of different images classifications like digit classifier and traffic sign predictions among others.</p>
<h5 id="model-experimentation">Model Experimentation</h5>
<p>This process took more time than others since different settings and combination of parameters were performed to pursue the desired results.</p>
<p>Different assumptions were made in this step, such as including  different color spaces in the experiments to see if there are some channels that can better be consumed by the ConvNet and get betters results.</p>
<p>Changing the architecture of the ConvNet and adjusting hyper parameters with the aim of better accuracies were performed in this step.</p>
<h5 id="model-refinement">Model Refinement</h5>
<p>This is an extension of model experimentation that were performed by revisiting the results of the experiments, looking for hints that lead to extra parameters or architecture tuning.</p>
<h5 id="evaluation">Evaluation</h5>
<p>The evaluation was performed through the experimentation process after every training the test set were used to evaluate the model.</p>
<p>A better, final solution, despite all of the efforts to reach a very good model that does not overfit the test set, was not possible.</p>
<h3 id="improvement">Improvement</h3>
<p>There are several improvements that were considered in this project but were not explored due to scope of time or missing knowledge to integrate on this solution.</p>
<h4 id="more-image-preprocessing">More image preprocessing</h4>
<p>Considering this data set are images from cervix an improvement would be to have some preprocessing techniques to find regions of interest within these images and to discard the noise elements present on the images, like metal tools.</p>
<h4 id="applying-more-complex-convnet-architectures">Applying more complex ConvNet Architectures</h4>
<p>During the process I tried to use pre-trained AlexNet architecture, having almost near results to the LetNet model; however, trying even more complex architectures like VGG-16 or ResNet could bring better results.</p>
<h4 id="getting-more-data">Getting more data</h4>
<p>One aspect of this project is that data is not enough to build a robust model, obtaining more data for analysis with more complex methods, like applying clustering or mixture models to get more insights of data variety, will be a very good option to explore. However getting data about cervix screening was difficult.</p>
<p>I’m sure having more time to study and apply the improvements above can lead to a better and more robust solution.</p>
<p>Thanks to Udacity Team and David Edelsohn for their insights and feedback</p>
<h2 id="references">References</h2>
<ol>
<li>
<p>Yann Lecun, &ldquo;LeNet-5, convolutional neural networks&rdquo;, <a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a></p>
</li>
<li>
<p>Data Augmentation Tool, <a href="https://github.com/codebox/image_augmentor">https://github.com/codebox/image_augmentor</a></p>
</li>
<li>
<p>AlexNet architecture, <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
</li>
</ol>

              


            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
            
            <div class="post-actions-wrap">
  <nav>
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--disabled">
          
            <i class="fa fa-angle-left"></i>
            <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
          </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--disabled">
          
            <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
            <i class="fa fa-angle-right"></i>
          </a>
        </li>
      
    </ul>
  </nav>
  <ul class="post-actions post-action-share">
    
      <li class="post-action hide-lg hide-md hide-sm">
        <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
          <i class="fa fa-share-alt"></i>
        </a>
      </li>
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=https%3a%2f%2fmoisesvw.github.io%2f2017%2fnovember%2fdeep-learning-on-lenet5%2f">
          <i class="fa fa-google-plus"></i>
        </a>
      </li>
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fmoisesvw.github.io%2f2017%2fnovember%2fdeep-learning-on-lenet5%2f">
          <i class="fa fa-facebook-official"></i>
        </a>
      </li>
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fmoisesvw.github.io%2f2017%2fnovember%2fdeep-learning-on-lenet5%2f">
          <i class="fa fa-twitter"></i>
        </a>
      </li>
    
  </ul>
</div>


            
  


          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2023 Moises Vargas. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="2">
        <div class="post-actions-wrap">
  <nav>
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--disabled">
          
            <i class="fa fa-angle-left"></i>
            <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
          </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--disabled">
          
            <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
            <i class="fa fa-angle-right"></i>
          </a>
        </li>
      
    </ul>
  </nav>
  <ul class="post-actions post-action-share">
    
      <li class="post-action hide-lg hide-md hide-sm">
        <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
          <i class="fa fa-share-alt"></i>
        </a>
      </li>
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=https%3a%2f%2fmoisesvw.github.io%2f2017%2fnovember%2fdeep-learning-on-lenet5%2f">
          <i class="fa fa-google-plus"></i>
        </a>
      </li>
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fmoisesvw.github.io%2f2017%2fnovember%2fdeep-learning-on-lenet5%2f">
          <i class="fa fa-facebook-official"></i>
        </a>
      </li>
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fmoisesvw.github.io%2f2017%2fnovember%2fdeep-learning-on-lenet5%2f">
          <i class="fa fa-twitter"></i>
        </a>
      </li>
    
  </ul>
</div>


      </div>
      

    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-times"></i>
    </div>
    
    <h4 id="about-card-name">Moisés Vargas</h4>
    
      <div id="about-card-bio">Life Long Learner</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Machine Learning Engineer
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker-alt"></i>
        <br/>
        Medellín
      </div>
    
  </div>
</div>

    

    
  
    
      
      <div id="cover" style="background-image:url('https://moisesvw.github.io/images/sfbridge_.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


<script src="https://moisesvw.github.io/js/script-yqzy9wdlzix4lbbwdnzvwx3egsne77earqmn73v9uno8aupuph8wfguccut.min.js"></script>






    
  </body>
</html>

